{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UO_e4YzCIIY6"
   },
   "source": [
    "This file processes the provided data `tweets.csv`. It is downloaded from https://dataverse.harvard.edu/dataset.xhtml?id=3047332."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8Ua_dRmRB0yk"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>country</th>\n",
       "      <th>date_time</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>number_of_likes</th>\n",
       "      <th>number_of_shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/01/2017 19:52</td>\n",
       "      <td>8.196330e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7900</td>\n",
       "      <td>3472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/01/2017 08:38</td>\n",
       "      <td>8.191010e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3689</td>\n",
       "      <td>1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/01/2017 02:52</td>\n",
       "      <td>8.190140e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10341</td>\n",
       "      <td>2387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/01/2017 02:44</td>\n",
       "      <td>8.190120e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10774</td>\n",
       "      <td>2458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/01/2017 05:22</td>\n",
       "      <td>8.186890e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17620</td>\n",
       "      <td>4655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52537</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>Life couldn't be better right now. üòä</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06/01/2015 23:10</td>\n",
       "      <td>5.526030e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32799</td>\n",
       "      <td>23796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52538</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>First Monday back in action. I'd say 21.6 mile...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06/01/2015 02:17</td>\n",
       "      <td>5.522880e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21709</td>\n",
       "      <td>12511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52539</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>Crime shows, buddy, snuggles = the perfect Sun...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/01/2015 03:42</td>\n",
       "      <td>5.519470e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25269</td>\n",
       "      <td>15583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52540</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>‚ùÑÔ∏è http://t.co/sHCFdPpGPa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/01/2015 00:06</td>\n",
       "      <td>5.518920e+17</td>\n",
       "      <td>und</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15985</td>\n",
       "      <td>10456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52541</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>‚ù§Ô∏è‚ùÑÔ∏è‚úàÔ∏è http://t.co/ixmB5lv17Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/01/2015 00:02</td>\n",
       "      <td>5.518910e+17</td>\n",
       "      <td>und</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16193</td>\n",
       "      <td>10822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52542 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          author                                            content country  \\\n",
       "0      katyperry  Is history repeating itself...?#DONTNORMALIZEH...     NaN   \n",
       "1      katyperry  @barackobama Thank you for your incredible gra...     NaN   \n",
       "2      katyperry                Life goals. https://t.co/XIn1qKMKQl     NaN   \n",
       "3      katyperry            Me right now üôèüèª https://t.co/gW55C1wrwd     NaN   \n",
       "4      katyperry  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...     NaN   \n",
       "...          ...                                                ...     ...   \n",
       "52537   ddlovato               Life couldn't be better right now. üòä     NaN   \n",
       "52538   ddlovato  First Monday back in action. I'd say 21.6 mile...     NaN   \n",
       "52539   ddlovato  Crime shows, buddy, snuggles = the perfect Sun...     NaN   \n",
       "52540   ddlovato                          ‚ùÑÔ∏è http://t.co/sHCFdPpGPa     NaN   \n",
       "52541   ddlovato                      ‚ù§Ô∏è‚ùÑÔ∏è‚úàÔ∏è http://t.co/ixmB5lv17Z     NaN   \n",
       "\n",
       "              date_time            id language  latitude  longitude  \\\n",
       "0      12/01/2017 19:52  8.196330e+17       en       NaN        NaN   \n",
       "1      11/01/2017 08:38  8.191010e+17       en       NaN        NaN   \n",
       "2      11/01/2017 02:52  8.190140e+17       en       NaN        NaN   \n",
       "3      11/01/2017 02:44  8.190120e+17       en       NaN        NaN   \n",
       "4      10/01/2017 05:22  8.186890e+17       en       NaN        NaN   \n",
       "...                 ...           ...      ...       ...        ...   \n",
       "52537  06/01/2015 23:10  5.526030e+17       en       NaN        NaN   \n",
       "52538  06/01/2015 02:17  5.522880e+17       en       NaN        NaN   \n",
       "52539  05/01/2015 03:42  5.519470e+17       en       NaN        NaN   \n",
       "52540  05/01/2015 00:06  5.518920e+17      und       NaN        NaN   \n",
       "52541  05/01/2015 00:02  5.518910e+17      und       NaN        NaN   \n",
       "\n",
       "       number_of_likes  number_of_shares  \n",
       "0                 7900              3472  \n",
       "1                 3689              1380  \n",
       "2                10341              2387  \n",
       "3                10774              2458  \n",
       "4                17620              4655  \n",
       "...                ...               ...  \n",
       "52537            32799             23796  \n",
       "52538            21709             12511  \n",
       "52539            25269             15583  \n",
       "52540            15985             10456  \n",
       "52541            16193             10822  \n",
       "\n",
       "[52542 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv(\"tweets.csv\")\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tr6k3V66CMoB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(tweets_df['content'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oydhAntrCOp9"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(tweets, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "labels = torch.tensor(len(tweets_df['author'][:100])).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EaNr0kGpHCJD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "input_ids = torch.tensor([tokenizer.encode(tweets[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Xtkg8GoVItm0"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = bertweet(inputs.input_ids)  # Models outputs are now tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "A8-8vjgmMEo2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHDni4DiS1iT"
   },
   "source": [
    "## Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "WsQbM5lCH2TF"
   },
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "        self.bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        inputs = tokenizer(tweets, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        labels = torch.tensor(len(df['author'])).unsqueeze(0)\n",
    "\n",
    "        # Run forward pass to get embeddings\n",
    "        with torch.no_grad():\n",
    "            self.features = bertweet(inputs.input_ids)  # Models outputs are now tuples\n",
    "\n",
    "        # Get pooled embeddings \n",
    "        xs = features.pooler_output\n",
    "        ys = torch.tensor(len(tweets_df['author'][:100])).unsqueeze(0)\n",
    "\n",
    "        return (xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "gvsIJ2uGOZnA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, df, batch_size=1000):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "        self.bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "        self.inputs = tokenizer(tweets, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        self.labels = torch.tensor(len(df['author'])).unsqueeze(0)\n",
    "        \n",
    "        \n",
    "        self.input_ids = torch.tensor([tokenizer.encode(tweets[0])])\n",
    "\n",
    "        embeddings = features.pooler_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Run forward pass to get embeddings\n",
    "        with torch.no_grad():\n",
    "            features = bertweet(self.inputs.input_ids)\n",
    "        \n",
    "        return features[idx], self[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ds = TweetDataset(tweets_df, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([52542])\n"
     ]
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gf/79nn5nv978s1wbswtcngq_800000gn/T/ipykernel_7446/2312964734.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Principal Components Analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Get x, y and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute_name)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Principal Components Analysis\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(ds.embeddings.t())\n",
    "\n",
    "# Get x, y and labels\n",
    "x, y = pca.components_\n",
    "names = tweets_df['author']\n",
    "\n",
    "# Use seaborn to plot\n",
    "fig = plt.figure(figsize = (12,8), dpi = 300)\n",
    "g = sns.scatterplot(x, y, hue = names)\n",
    "g.legend(loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bootcamp Day 2.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
