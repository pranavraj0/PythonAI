{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# if this fails, open terminal (on Linux, Mac) or Anaconda Prompt (windows)\n",
    "# and run:\n",
    "# conda install -c huggingface transformers\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(\"data/tweets.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['katyperry' 'justinbieber' 'taylorswift13' 'BarackObama' 'rihanna'\n",
      " 'YouTube' 'ladygaga' 'TheEllenShow' 'Twitter' 'jtimberlake'\n",
      " 'KimKardashian' 'britneyspears' 'Cristiano' 'selenagomez' 'cnnbrk'\n",
      " 'jimmyfallon' 'ArianaGrande' 'shakira' 'instagram' 'ddlovato']\n"
     ]
    }
   ],
   "source": [
    "unique_people = tweets_df['author'].unique()\n",
    "print(unique_people)\n",
    "NUM_CLASSES = len(unique_people)\n",
    "\n",
    "# assign each person a number\n",
    "id_to_person = {i: unique_people[i] for i in range(len(unique_people))}\n",
    "person_to_id = {v:k for k,v in id_to_person.items()}\n",
    "\n",
    "# create a column of author ids\n",
    "tweets_df['author_id'] = tweets_df['author'].apply(lambda x: person_to_id[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>country</th>\n",
       "      <th>date_time</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>number_of_likes</th>\n",
       "      <th>number_of_shares</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/01/2017 19:52</td>\n",
       "      <td>8.196330e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7900</td>\n",
       "      <td>3472</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/01/2017 08:38</td>\n",
       "      <td>8.191010e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3689</td>\n",
       "      <td>1380</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/01/2017 02:52</td>\n",
       "      <td>8.190140e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10341</td>\n",
       "      <td>2387</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/01/2017 02:44</td>\n",
       "      <td>8.190120e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10774</td>\n",
       "      <td>2458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/01/2017 05:22</td>\n",
       "      <td>8.186890e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17620</td>\n",
       "      <td>4655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52537</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>Life couldn't be better right now. üòä</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06/01/2015 23:10</td>\n",
       "      <td>5.526030e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32799</td>\n",
       "      <td>23796</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52538</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>First Monday back in action. I'd say 21.6 mile...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06/01/2015 02:17</td>\n",
       "      <td>5.522880e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21709</td>\n",
       "      <td>12511</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52539</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>Crime shows, buddy, snuggles = the perfect Sun...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/01/2015 03:42</td>\n",
       "      <td>5.519470e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25269</td>\n",
       "      <td>15583</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52540</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>‚ùÑÔ∏è http://t.co/sHCFdPpGPa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/01/2015 00:06</td>\n",
       "      <td>5.518920e+17</td>\n",
       "      <td>und</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15985</td>\n",
       "      <td>10456</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52541</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>‚ù§Ô∏è‚ùÑÔ∏è‚úàÔ∏è http://t.co/ixmB5lv17Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/01/2015 00:02</td>\n",
       "      <td>5.518910e+17</td>\n",
       "      <td>und</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16193</td>\n",
       "      <td>10822</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52542 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          author                                            content country  \\\n",
       "0      katyperry  Is history repeating itself...?#DONTNORMALIZEH...     NaN   \n",
       "1      katyperry  @barackobama Thank you for your incredible gra...     NaN   \n",
       "2      katyperry                Life goals. https://t.co/XIn1qKMKQl     NaN   \n",
       "3      katyperry            Me right now üôèüèª https://t.co/gW55C1wrwd     NaN   \n",
       "4      katyperry  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...     NaN   \n",
       "...          ...                                                ...     ...   \n",
       "52537   ddlovato               Life couldn't be better right now. üòä     NaN   \n",
       "52538   ddlovato  First Monday back in action. I'd say 21.6 mile...     NaN   \n",
       "52539   ddlovato  Crime shows, buddy, snuggles = the perfect Sun...     NaN   \n",
       "52540   ddlovato                          ‚ùÑÔ∏è http://t.co/sHCFdPpGPa     NaN   \n",
       "52541   ddlovato                      ‚ù§Ô∏è‚ùÑÔ∏è‚úàÔ∏è http://t.co/ixmB5lv17Z     NaN   \n",
       "\n",
       "              date_time            id language  latitude  longitude  \\\n",
       "0      12/01/2017 19:52  8.196330e+17       en       NaN        NaN   \n",
       "1      11/01/2017 08:38  8.191010e+17       en       NaN        NaN   \n",
       "2      11/01/2017 02:52  8.190140e+17       en       NaN        NaN   \n",
       "3      11/01/2017 02:44  8.190120e+17       en       NaN        NaN   \n",
       "4      10/01/2017 05:22  8.186890e+17       en       NaN        NaN   \n",
       "...                 ...           ...      ...       ...        ...   \n",
       "52537  06/01/2015 23:10  5.526030e+17       en       NaN        NaN   \n",
       "52538  06/01/2015 02:17  5.522880e+17       en       NaN        NaN   \n",
       "52539  05/01/2015 03:42  5.519470e+17       en       NaN        NaN   \n",
       "52540  05/01/2015 00:06  5.518920e+17      und       NaN        NaN   \n",
       "52541  05/01/2015 00:02  5.518910e+17      und       NaN        NaN   \n",
       "\n",
       "       number_of_likes  number_of_shares  author_id  \n",
       "0                 7900              3472          0  \n",
       "1                 3689              1380          0  \n",
       "2                10341              2387          0  \n",
       "3                10774              2458          0  \n",
       "4                17620              4655          0  \n",
       "...                ...               ...        ...  \n",
       "52537            32799             23796         19  \n",
       "52538            21709             12511         19  \n",
       "52539            25269             15583         19  \n",
       "52540            15985             10456         19  \n",
       "52541            16193             10822         19  \n",
       "\n",
       "[52542 rows x 11 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        print('making tokenizer...')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "\n",
    "        print('making bert...')\n",
    "        self.bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "        print('tokenizing inputs...')\n",
    "        self.inputs = self.tokenizer(list(df['content']), return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        \n",
    "        # Run BERT forward pass to get embeddings\n",
    "        print('running BERT to get embeddings...')\n",
    "        with torch.no_grad():\n",
    "            features = self.bertweet(self.inputs.input_ids)\n",
    "        \n",
    "        self.embeddings = features.pooler_output\n",
    "        self.labels = torch.tensor(df['author_id'].values).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = self.embeddings[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making bert...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing inputs...\n",
      "running BERT to get embeddings...\n",
      "making tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making bert...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing inputs...\n",
      "running BERT to get embeddings...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gf/79nn5nv978s1wbswtcngq_800000gn/T/ipykernel_10200/95533816.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mds_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTweetDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mds_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTweetDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/gf/79nn5nv978s1wbswtcngq_800000gn/T/ipykernel_10200/1929957542.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'running BERT to get embeddings...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbertweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    813\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         )\n\u001b[0;32m--> 815\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    816\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    506\u001b[0m                 )\n\u001b[1;32m    507\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    509\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1990\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1992\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# shuffle the df\n",
    "np.random.seed(7)\n",
    "df_full = tweets_df.sample(frac=1.0) #random\n",
    "\n",
    "# only do 100/100 for the sake of time; for the sake of testing things you haven't seen before\n",
    "n_train = 250\n",
    "n_test = 250\n",
    "\n",
    "df_train = df_full[:n_train]\n",
    "df_test = df_full[n_train:n_train+n_test]\n",
    "\n",
    "ds_train = TweetDataset(df_train)\n",
    "ds_test = TweetDataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7)\n",
    "\n",
    "#1 Hidden Layer; softmax creates probability distribution\n",
    "\n",
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(768, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, NUM_CLASSES),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "loss_f = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(ds):\n",
    "    num_correct = 0\n",
    "\n",
    "    batch_start = 0\n",
    "    batch_end = batch_start + batch_size\n",
    "    while batch_start < len(ds):\n",
    "        # Grab the batch from the dataset\n",
    "        x, y = ds[batch_start:batch_end]\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            y_pred = mlp(x)\n",
    "\n",
    "        # Gets the number of correct predictions\n",
    "        num_correct += (torch.argmax(y_pred, dim=1) == y).numpy().sum()\n",
    "\n",
    "        # Update batch parameters\n",
    "        batch_start = batch_end\n",
    "        batch_end = batch_start + batch_size\n",
    "    \n",
    "    return num_correct / len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle the indices\n",
    "idxs = np.arange(len(ds_train))\n",
    "np.random.shuffle(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      "Train Acc on Epoch 0: 0.04\n",
      "Test Acc on Epoch 0: 0.04\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 1: 0.07\n",
      "Test Acc on Epoch 1: 0.03\n",
      "----\n",
      "Epoch 2, Batch 0. Train loss: 2.9944939613342285\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 2: 0.07\n",
      "Test Acc on Epoch 2: 0.03\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 3: 0.09\n",
      "Test Acc on Epoch 3: 0.05\n",
      "----\n",
      "Epoch 4, Batch 1. Train loss: 2.9909017086029053\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 4: 0.09\n",
      "Test Acc on Epoch 4: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 5: 0.09\n",
      "Test Acc on Epoch 5: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 6: 0.09\n",
      "Test Acc on Epoch 6: 0.05\n",
      "----\n",
      "Epoch 7, Batch 0. Train loss: 2.9874918460845947\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 7: 0.09\n",
      "Test Acc on Epoch 7: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 8: 0.09\n",
      "Test Acc on Epoch 8: 0.05\n",
      "----\n",
      "Epoch 9, Batch 1. Train loss: 2.9703545570373535\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 9: 0.09\n",
      "Test Acc on Epoch 9: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 10: 0.09\n",
      "Test Acc on Epoch 10: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 11: 0.09\n",
      "Test Acc on Epoch 11: 0.05\n",
      "----\n",
      "Epoch 12, Batch 0. Train loss: 2.980980634689331\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 12: 0.09\n",
      "Test Acc on Epoch 12: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 13: 0.09\n",
      "Test Acc on Epoch 13: 0.05\n",
      "----\n",
      "Epoch 14, Batch 1. Train loss: 2.9443845748901367\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 14: 0.09\n",
      "Test Acc on Epoch 14: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 15: 0.09\n",
      "Test Acc on Epoch 15: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 16: 0.09\n",
      "Test Acc on Epoch 16: 0.05\n",
      "----\n",
      "Epoch 17, Batch 0. Train loss: 2.9875288009643555\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 17: 0.09\n",
      "Test Acc on Epoch 17: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 18: 0.09\n",
      "Test Acc on Epoch 18: 0.05\n",
      "----\n",
      "Epoch 19, Batch 1. Train loss: 2.9407198429107666\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 19: 0.09\n",
      "Test Acc on Epoch 19: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 20: 0.09\n",
      "Test Acc on Epoch 20: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 21: 0.09\n",
      "Test Acc on Epoch 21: 0.05\n",
      "----\n",
      "Epoch 22, Batch 0. Train loss: 2.9846127033233643\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 22: 0.09\n",
      "Test Acc on Epoch 22: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 23: 0.09\n",
      "Test Acc on Epoch 23: 0.05\n",
      "----\n",
      "Epoch 24, Batch 1. Train loss: 2.9418630599975586\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 24: 0.09\n",
      "Test Acc on Epoch 24: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 25: 0.09\n",
      "Test Acc on Epoch 25: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 26: 0.09\n",
      "Test Acc on Epoch 26: 0.05\n",
      "----\n",
      "Epoch 27, Batch 0. Train loss: 2.9849321842193604\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 27: 0.09\n",
      "Test Acc on Epoch 27: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 28: 0.09\n",
      "Test Acc on Epoch 28: 0.05\n",
      "----\n",
      "Epoch 29, Batch 1. Train loss: 2.938096046447754\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 29: 0.09\n",
      "Test Acc on Epoch 29: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 30: 0.09\n",
      "Test Acc on Epoch 30: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 31: 0.09\n",
      "Test Acc on Epoch 31: 0.05\n",
      "----\n",
      "Epoch 32, Batch 0. Train loss: 2.9877309799194336\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 32: 0.09\n",
      "Test Acc on Epoch 32: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 33: 0.09\n",
      "Test Acc on Epoch 33: 0.05\n",
      "----\n",
      "Epoch 34, Batch 1. Train loss: 2.9341564178466797\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 34: 0.09\n",
      "Test Acc on Epoch 34: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 35: 0.09\n",
      "Test Acc on Epoch 35: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 36: 0.09\n",
      "Test Acc on Epoch 36: 0.05\n",
      "----\n",
      "Epoch 37, Batch 0. Train loss: 2.9900918006896973\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 37: 0.09\n",
      "Test Acc on Epoch 37: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 38: 0.09\n",
      "Test Acc on Epoch 38: 0.05\n",
      "----\n",
      "Epoch 39, Batch 1. Train loss: 2.9318323135375977\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 39: 0.09\n",
      "Test Acc on Epoch 39: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 40: 0.09\n",
      "Test Acc on Epoch 40: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 41: 0.09\n",
      "Test Acc on Epoch 41: 0.05\n",
      "----\n",
      "Epoch 42, Batch 0. Train loss: 2.990884304046631\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 42: 0.09\n",
      "Test Acc on Epoch 42: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 43: 0.09\n",
      "Test Acc on Epoch 43: 0.05\n",
      "----\n",
      "Epoch 44, Batch 1. Train loss: 2.930988073348999\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 44: 0.09\n",
      "Test Acc on Epoch 44: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 45: 0.09\n",
      "Test Acc on Epoch 45: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 46: 0.09\n",
      "Test Acc on Epoch 46: 0.05\n",
      "----\n",
      "Epoch 47, Batch 0. Train loss: 2.9903690814971924\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 47: 0.09\n",
      "Test Acc on Epoch 47: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 48: 0.09\n",
      "Test Acc on Epoch 48: 0.05\n",
      "----\n",
      "Epoch 49, Batch 1. Train loss: 2.930771827697754\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 49: 0.09\n",
      "Test Acc on Epoch 49: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 50: 0.09\n",
      "Test Acc on Epoch 50: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 51: 0.09\n",
      "Test Acc on Epoch 51: 0.05\n",
      "----\n",
      "Epoch 52, Batch 0. Train loss: 2.9894964694976807\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 52: 0.09\n",
      "Test Acc on Epoch 52: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 53: 0.09\n",
      "Test Acc on Epoch 53: 0.05\n",
      "----\n",
      "Epoch 54, Batch 1. Train loss: 2.930155038833618\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 54: 0.09\n",
      "Test Acc on Epoch 54: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 55: 0.09\n",
      "Test Acc on Epoch 55: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 56: 0.09\n",
      "Test Acc on Epoch 56: 0.05\n",
      "----\n",
      "Epoch 57, Batch 0. Train loss: 2.988956928253174\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 57: 0.09\n",
      "Test Acc on Epoch 57: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 58: 0.09\n",
      "Test Acc on Epoch 58: 0.05\n",
      "----\n",
      "Epoch 59, Batch 1. Train loss: 2.9287304878234863\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 59: 0.09\n",
      "Test Acc on Epoch 59: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 60: 0.09\n",
      "Test Acc on Epoch 60: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 61: 0.09\n",
      "Test Acc on Epoch 61: 0.05\n",
      "----\n",
      "Epoch 62, Batch 0. Train loss: 2.9885475635528564\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 62: 0.09\n",
      "Test Acc on Epoch 62: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 63: 0.09\n",
      "Test Acc on Epoch 63: 0.05\n",
      "----\n",
      "Epoch 64, Batch 1. Train loss: 2.9266855716705322\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 64: 0.09\n",
      "Test Acc on Epoch 64: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 65: 0.09\n",
      "Test Acc on Epoch 65: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 66: 0.09\n",
      "Test Acc on Epoch 66: 0.05\n",
      "----\n",
      "Epoch 67, Batch 0. Train loss: 2.9879090785980225\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 67: 0.09\n",
      "Test Acc on Epoch 67: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 68: 0.09\n",
      "Test Acc on Epoch 68: 0.05\n",
      "----\n",
      "Epoch 69, Batch 1. Train loss: 2.9241111278533936\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 69: 0.1\n",
      "Test Acc on Epoch 69: 0.06\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 70: 0.1\n",
      "Test Acc on Epoch 70: 0.06\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 71: 0.11\n",
      "Test Acc on Epoch 71: 0.06\n",
      "----\n",
      "Epoch 72, Batch 0. Train loss: 2.986937999725342\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 72: 0.11\n",
      "Test Acc on Epoch 72: 0.06\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 73: 0.11\n",
      "Test Acc on Epoch 73: 0.06\n",
      "----\n",
      "Epoch 74, Batch 1. Train loss: 2.9197404384613037\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 74: 0.11\n",
      "Test Acc on Epoch 74: 0.06\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 75: 0.11\n",
      "Test Acc on Epoch 75: 0.06\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 76: 0.12\n",
      "Test Acc on Epoch 76: 0.06\n",
      "----\n",
      "Epoch 77, Batch 0. Train loss: 2.9856715202331543\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 77: 0.12\n",
      "Test Acc on Epoch 77: 0.06\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 78: 0.12\n",
      "Test Acc on Epoch 78: 0.06\n",
      "----\n",
      "Epoch 79, Batch 1. Train loss: 2.914163589477539\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 79: 0.12\n",
      "Test Acc on Epoch 79: 0.06\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 80: 0.12\n",
      "Test Acc on Epoch 80: 0.06\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 81: 0.12\n",
      "Test Acc on Epoch 81: 0.06\n",
      "----\n",
      "Epoch 82, Batch 0. Train loss: 2.9838907718658447\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 82: 0.12\n",
      "Test Acc on Epoch 82: 0.06\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 83: 0.12\n",
      "Test Acc on Epoch 83: 0.06\n",
      "----\n",
      "Epoch 84, Batch 1. Train loss: 2.9069106578826904\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 84: 0.12\n",
      "Test Acc on Epoch 84: 0.06\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 85: 0.12\n",
      "Test Acc on Epoch 85: 0.06\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 86: 0.13\n",
      "Test Acc on Epoch 86: 0.06\n",
      "----\n",
      "Epoch 87, Batch 0. Train loss: 2.9805970191955566\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 87: 0.13\n",
      "Test Acc on Epoch 87: 0.06\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 88: 0.13\n",
      "Test Acc on Epoch 88: 0.06\n",
      "----\n",
      "Epoch 89, Batch 1. Train loss: 2.8985588550567627\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 89: 0.13\n",
      "Test Acc on Epoch 89: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 90: 0.13\n",
      "Test Acc on Epoch 90: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 91: 0.13\n",
      "Test Acc on Epoch 91: 0.05\n",
      "----\n",
      "Epoch 92, Batch 0. Train loss: 2.977743148803711\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 92: 0.13\n",
      "Test Acc on Epoch 92: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 93: 0.13\n",
      "Test Acc on Epoch 93: 0.05\n",
      "----\n",
      "Epoch 94, Batch 1. Train loss: 2.8906610012054443\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 94: 0.13\n",
      "Test Acc on Epoch 94: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 95: 0.13\n",
      "Test Acc on Epoch 95: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 96: 0.13\n",
      "Test Acc on Epoch 96: 0.05\n",
      "----\n",
      "Epoch 97, Batch 0. Train loss: 2.974595308303833\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 97: 0.13\n",
      "Test Acc on Epoch 97: 0.05\n",
      "----\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 98: 0.13\n",
      "Test Acc on Epoch 98: 0.05\n",
      "----\n",
      "Epoch 99, Batch 1. Train loss: 2.8832345008850098\n",
      "\n",
      "----\n",
      "Train Acc on Epoch 99: 0.13\n",
      "Test Acc on Epoch 99: 0.05\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "mlp.train()\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "# print every these number of batches\n",
    "print_frequency = 5\n",
    "print_counter = print_frequency\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(epochs):\n",
    "    batch_start = 0\n",
    "    batch_end = batch_start + batch_size\n",
    "    while batch_start < len(ds_train):\n",
    "        # Grab the batch from the dataset\n",
    "        idxs_batch = idxs[batch_start:batch_end]\n",
    "        x, y = ds_train[idxs_batch]\n",
    "               \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = mlp(x)\n",
    "\n",
    "        # Compute Loss\n",
    "        loss = loss_f(y_pred, y)\n",
    "        \n",
    "        print_counter -= 1\n",
    "        if print_counter <= 0:\n",
    "            print_counter = print_frequency\n",
    "            \n",
    "            # Figure out which batch we are on\n",
    "            batch_num = batch_start // batch_size\n",
    "            print(f'Epoch {epoch}, Batch {batch_num}. Train loss: {loss.item()}')\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update batch parameters\n",
    "        batch_start = batch_end\n",
    "        batch_end = batch_start + batch_size\n",
    "    \n",
    "    print('\\n----')\n",
    "    \n",
    "    train_acc = get_accuracy(ds_train)\n",
    "    print(f'Train Acc on Epoch {epoch}:', train_acc)\n",
    "    \n",
    "    test_acc = get_accuracy(ds_test)\n",
    "    print(f'Test Acc on Epoch {epoch}:', test_acc)\n",
    "    \n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedddings(strings):\n",
    "    inputs = tokenizer(strings, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "    # Run BERT forward pass to get embeddings\n",
    "    with torch.no_grad():\n",
    "        features = bertweet(inputs.input_ids)\n",
    "    return features.pooler_output\n",
    "\n",
    "\n",
    "def get_predictions(embeddings):\n",
    "    with torch.no_grad():\n",
    "        preds_proba = mlp(embeddings).detach().numpy()\n",
    "    preds = np.argmax(preds_proba, axis=1)\n",
    "    print(preds)\n",
    "    \n",
    "    out = [] # tuples of (person, probability)\n",
    "    for i, p in enumerate(preds):\n",
    "        person = id_to_person[p]\n",
    "        prob = preds_proba[i,p]\n",
    "        out.append((person, prob))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbitrary_inputs = [\n",
    "    'i love coffee',\n",
    "    'tom brady deflates footballs',\n",
    "    'Anakin, you were my brother!',\n",
    "    'u up?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 13 13 13]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('selenagomez', 0.52321607),\n",
       " ('selenagomez', 0.46897084),\n",
       " ('selenagomez', 0.39001203),\n",
       " ('selenagomez', 0.53850484)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = get_embedddings(arbitrary_inputs)\n",
    "predictions = get_predictions(embeddings)\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
